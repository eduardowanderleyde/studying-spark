# ğŸ“š Estudando Apache Spark com PySpark

Projeto de estudos do Apache Spark usando PySpark em container Docker.

## ğŸ³ ConfiguraÃ§Ã£o Docker

Este projeto utiliza Docker Compose para rodar:
- **Jupyter Lab** com PySpark (Python 3.11)
- **MinIO** para armazenamento de objetos (S3-compatible)

### Como executar

```bash
docker-compose up -d
```

### Acessar serviÃ§os

- **Jupyter Lab**: http://localhost:8888 (senha: `password`)
- **Spark UI**: http://localhost:4050
- **MinIO**: http://localhost:9000

## ğŸ“ Estrutura do Projeto

```
studying-spark/
â”œâ”€â”€ dados/              # Arquivos de dados (CSV, etc)
â”œâ”€â”€ exemplos/           # Scripts Python com exemplos
â”œâ”€â”€ notebooks/          # Jupyter Notebooks
â””â”€â”€ docker-compose.yml  # ConfiguraÃ§Ã£o Docker
```

## ğŸš€ Exemplos

- `01_hello_spark.py` - IntroduÃ§Ã£o ao Spark
- `02_dataframes.py` - OperaÃ§Ãµes com DataFrames
- `03_spark_sql.py` - Consultas SQL no Spark
- `04_ler_csv.py` - Leitura e escrita de arquivos

## ğŸ’¡ Tecnologias

- Apache Spark 3.x
- Python 3.11
- PySpark
- Jupyter Lab
- Docker
- MinIO

